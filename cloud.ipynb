{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba28db67-d77c-49ed-96cd-1f2177e7ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.nn import Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a701cbf-1b94-4c76-ad2b-20e0c7b59062",
   "metadata": {},
   "source": [
    "## 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3471a3dc-1667-4ac5-9181-3c7ca19e2449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efdc9e9e8f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "cloud_num = 28\n",
    "focal_loss_gmma = 3.0\n",
    "epochs = 100\n",
    "model_name = 'resnet-50'\n",
    "optimizer_name = 'radam'\n",
    "scheduler_name = 'cosine_annealing_warm_restarts'\n",
    "# scheduler_name = 'cosine_annealing'\n",
    "# scheduler_name = 'none'\n",
    "T_0 = 15\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d22b4d-38c9-47e8-b8d1-b4458f67b0b4",
   "metadata": {},
   "source": [
    "## 模型加载\n",
    "\n",
    "使用`torchvision.models`中的resnet50作为预训练模型，并修改其全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e02ba30-e02e-472b-bfca-97e173ac22d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(cloud_class_nums):\n",
    "    pretrained_model = models.resnet50(weights=models.resnet.ResNet50_Weights.DEFAULT)\n",
    "    # pretrained_model = models.resnet101(weights=models.resnet.ResNet101_Weights.DEFAULT)\n",
    "    num_ftrs = pretrained_model.fc.in_features\n",
    "    pretrained_model.fc = Linear(num_ftrs, cloud_class_nums)\n",
    "    return pretrained_model\n",
    "\n",
    "model = load_model(cloud_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a5517-a516-47b8-aaff-0027fdf8bde3",
   "metadata": {},
   "source": [
    "## 构建数据集类\n",
    "根据`torch`官方文档修正`transform`参数:\n",
    "- 训练阶段:将图片随机切分并`resize`到$224 \\times 224$\n",
    "- 验证阶段:\n",
    "  - 将图片`resize`到$256 \\times 256$\n",
    "  - 截取`center`的$24 \\times 224$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf3de80b-39c3-4987-877d-948ab880d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from utils.autoaugment import ImageNetPolicy\n",
    "\n",
    "class CloudImageDataSet(Dataset):\n",
    "    def __init__(self, img_dir_path, img_set_with_label: dict, data_transforms=None, dataset = ''):\n",
    "        self.imgs = list(img_set_with_label.keys())\n",
    "        self.labels = list(img_set_with_label.values())\n",
    "        self.data_transforms = data_transforms\n",
    "        self.img_dir_path = img_dir_path\n",
    "        self.dataset = dataset\n",
    "        tmp_imgs = []\n",
    "        tmp_labels = []\n",
    "        if dataset == 'Train':\n",
    "            print('Train Upsampling')\n",
    "            for i, label in enumerate(self.labels):\n",
    "                '''upsampling'''\n",
    "                if label == 6 or label == 12:\n",
    "                    for _ in range(9):\n",
    "                        tmp_labels.append(self.labels[i])\n",
    "                        tmp_imgs.append(self.imgs[i])\n",
    "                tmp_labels.append(self.labels[i])\n",
    "                tmp_imgs.append(self.imgs[i])\n",
    "            self.labels = tmp_labels\n",
    "            self.imgs = tmp_imgs\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, k):\n",
    "        img_name = self.imgs[k]\n",
    "        label = torch.nn.functional.one_hot(torch.tensor(self.labels[k] - 1), num_classes=cloud_num).double()\n",
    "        img_path = os.path.join(self.img_dir_path, img_name)\n",
    "        with open(img_path, 'rb') as img_file:\n",
    "            img = Image.open(img_file)\n",
    "            img = img.convert('RGB')\n",
    "        img_file.close()\n",
    "        return self.data_transforms[self.dataset](img), label\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c21778da-5579-4cd7-8b78-1e7059dda83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89093d77-8440-4cf6-8094-8cb081498f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas\n",
    "\n",
    "train_dataset_df = pandas.read_csv('./cloud_dataset/train.csv').T\n",
    "train_image_name_and_label_set = {}\n",
    "for k, v in train_dataset_df.to_dict().items():\n",
    "    train_image_name_and_label_set[v['FileName']] = v['Code']\n",
    "val_dataset_df = pandas.read_csv('./cloud_dataset/standard_answer.csv').T\n",
    "val_image_name_and_label_set = {}\n",
    "for k, v in val_dataset_df.to_dict().items():\n",
    "    val_image_name_and_label_set[v['FileName']] = v['Code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf46454d-87ea-4a41-a279-56e6e9f2510f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ttach as tta\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d355c-2626-4b79-90b0-c249d1b7253f",
   "metadata": {},
   "source": [
    "## Focal Loss\n",
    "\n",
    "$-(1-p_t)log(p_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b1da06b-3421-4ce4-9fd1-e9ab2da3a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Softmax, CrossEntropyLoss\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gmma):\n",
    "        super().__init__()\n",
    "        self.gmma = gmma\n",
    "        self.softmax = Softmax(dim=1)\n",
    "        self.cross_entropy_loss = CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, pred, label):\n",
    "        pred_after_softmax = self.softmax(pred)\n",
    "        ce = - torch.log(pred_after_softmax) * label\n",
    "        focal_loss = (((1 - pred_after_softmax * label) ** self.gmma) * ce).sum() / batch_size\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fab05630-751a-400e-a552-f76da18a18c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: adjusting learning rate of group 0 to 1.0000e-03.\n"
     ]
    }
   ],
   "source": [
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss_fn = FocalLoss(gmma=focal_loss_gmma)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.RAdam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=T_0, T_mult=1, verbose=True)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_0, verbose=True, eta_min=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953cf192-95ff-446b-acaf-e42e5a406f42",
   "metadata": {},
   "source": [
    "## 训练\n",
    "将`test.csv`中的数据加载，进行`K-Fold`训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "712950f1-82ec-435f-ba3c-f90134cab275",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m timestamp \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     16\u001b[0m f, (ax1, ax2) \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, sharey\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     20\u001b[0m     data_transforms \u001b[39m=\u001b[39m {\n\u001b[1;32m     21\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m'\u001b[39m: transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m     22\u001b[0m             transforms\u001b[39m.\u001b[39mResize(\u001b[39mmax\u001b[39m(\u001b[39mint\u001b[39m(\u001b[39m224\u001b[39m \u001b[39m*\u001b[39m (epoch \u001b[39m+\u001b[39m \u001b[39m15\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m20\u001b[39m), \u001b[39m575\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m         ])\n\u001b[1;32m     36\u001b[0m     }\n\u001b[1;32m     37\u001b[0m     train_dataset \u001b[39m=\u001b[39m CloudImageDataSet(\u001b[39m'\u001b[39m\u001b[39m./cloud_dataset/images/\u001b[39m\u001b[39m'\u001b[39m, train_image_name_and_label_set, data_transforms, \u001b[39m'\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "max_f1_score = 0\n",
    "max_f1_score_model_name = ''\n",
    "model_f1_set = []\n",
    "model_avg_loss_set = []\n",
    "iteration = 0\n",
    "train_loss_set = []\n",
    "timestamp = time.time()\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    data_transforms = {\n",
    "        'Train': transforms.Compose([\n",
    "            transforms.Resize(max(int(224 * (epoch + 15) / 20), 575)),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            ImageNetPolicy(),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'Test': transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            # transforms.Resize(232), # resnet-101\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    }\n",
    "    train_dataset = CloudImageDataSet('./cloud_dataset/images/', train_image_name_and_label_set, data_transforms, 'Train')\n",
    "    val_dataset = CloudImageDataSet('./cloud_dataset/images', val_image_name_and_label_set, data_transforms, 'Test')\n",
    "    train_set, val_set = train_dataset, val_dataset\n",
    "    train_dataloader =  DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "    val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    # train_dataloader =  DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    # val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    size = len(train_dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 20 == 0:\n",
    "            iteration += 1\n",
    "            loss, current = loss.item(), batch * batch_size\n",
    "            train_loss_set.append(loss)\n",
    "            ax1.clear()\n",
    "            ax1.set_title('Train Loss')\n",
    "            ax1.plot([20 * _ for _ in range(iteration)], train_loss_set, label=\"loss\")\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(f)\n",
    "    test_loss, correct = 0, 0\n",
    "    size = len(val_dataloader.dataset)\n",
    "    num_batches = len(val_dataloader)\n",
    "    '''TODO:'''\n",
    "    if 10 < epoch < 23:\n",
    "        scheduler.step()\n",
    "    model.eval()\n",
    "    tta_model = tta.ClassificationTTAWrapper(model, tta.aliases.five_crop_transform(224, 224))\n",
    "    with torch.no_grad():\n",
    "        pred_set = torch.tensor([])\n",
    "        y_set = torch.tensor([])\n",
    "        for X, y in val_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = tta_model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            pred_set = torch.cat((pred_set, pred.cpu()), 0)\n",
    "            y_set = torch.cat((y_set, y.cpu()), 0)\n",
    "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    f1 = f1_score(pred_set.argmax(1), y_set.argmax(1), average=None).sum().item() / cloud_num\n",
    "    if f1 > max_f1_score:\n",
    "        max_f1_score = f1\n",
    "        max_f1_score_model_name = f'./model_state_dict/model-{batch_size}-{focal_loss_gmma}-{model_name}-epoch{epoch}-optimizer{optimizer_name}-scheduler{scheduler_name}-f1-score{f1}'\n",
    "    torch.save(model.state_dict(), f'./model_state_dict/model-{batch_size}-{focal_loss_gmma}-{model_name}-epoch{epoch}-optimizer{optimizer_name}-scheduler{scheduler_name}-f1-score{f1}')\n",
    "    log_file = open(f'./log/log-{batch_size}-{focal_loss_gmma}-{model_name}-optimizer{optimizer_name}-scheduler{scheduler_name}-time{timestamp}.txt', 'a')\n",
    "    print(f'epoch {epoch}---------------------------', file=log_file)\n",
    "    model_f1_set.append(f1)\n",
    "    model_avg_loss_set.append(test_loss)\n",
    "    ax2.clear()\n",
    "    ax2.plot(range(epoch + 1), model_f1_set, label=\"f1 score\")\n",
    "    ax2.plot(range(epoch + 1), model_avg_loss_set, label=\"avg loss\")\n",
    "    ax2.legend()\n",
    "    ax2.set_title('Test Loss&F1')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(f)\n",
    "    print(f\"Val: \\n Accuracy: {(100*correct):>0.1f}%, f1_score: {f1:>8f} Avg loss: {test_loss:>8f} \\n\", file=log_file)\n",
    "    log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a94bca1-b1a6-4aa0-925b-99dce841fe6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3860645842174864"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fadbbebf-8af9-4714-843f-af861e473b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_df = pandas.read_csv('/home/codefor/cloud-form/cloud_dataset/test.csv').T\n",
    "test_image_name_set = []\n",
    "for k, v in test_dataset_df.to_dict().items():\n",
    "    test_image_name_set.append(v['FileName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32752898-0742-4af8-89e8-88271a9eb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudImageDataSetForTest(Dataset):\n",
    "    def __init__(self, img_dir_path, img_set: list, data_transforms=None, dataset = ''):\n",
    "        self.imgs = img_set\n",
    "        self.data_transforms = data_transforms\n",
    "        self.img_dir_path = img_dir_path\n",
    "        self.dataset = dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, k):\n",
    "        img_name = self.imgs[k]\n",
    "        img_path = os.path.join(self.img_dir_path, img_name)\n",
    "        with open(img_path, 'rb') as img_file:\n",
    "            img = Image.open(img_file)\n",
    "            img = img.convert('RGB')\n",
    "        img_file.close()\n",
    "        return self.data_transforms[self.dataset](img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9438d2f-f6aa-4c53-87d3-205555f78997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "test_dataset = CloudImageDataSetForTest('./cloud_dataset/images/', test_image_name_set, data_transforms, 'Test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a9eae17-0208-4a8b-8446-97470c7a9c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "model.load_state_dict(torch.load('./model_state_dict/model-64-3.0-resnet-50-epoch13-optimizerradam-schedulercosine_annealing_warm_restarts-f1-score0.42401080509544153'))\n",
    "pred_set = torch.tensor([])\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pred_set = torch.tensor([])\n",
    "    for X in test_dataloader:\n",
    "        X = X.to(device)\n",
    "        pred = model(X)\n",
    "        pred_set = torch.cat((pred_set, pred.cpu().argmax(1) + 1), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2ad7fe0-bce9-4176-91c2-cd99bed3648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = zip(test_image_name_set, pred_set.int().tolist())\n",
    "submission = dict(submission)\n",
    "submission = pandas.DataFrame([submission]).T\n",
    "submission.to_csv('./cloud_dataset/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc1e7e47-d7fa-4b2b-a7b3-2fda6475707e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3860645842174864"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96cd7a39-090d-47af-9138-52219c8689d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_set = torch.tensor(val_dataset.labels)\n",
    "f1 = f1_score(pred_set, y_set, average=None).sum().item() / cloud_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5800f99b-b3ff-43de-802a-f85295b60087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Upsampling\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "data_transforms = {\n",
    "    'Test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        # transforms.Resize(232), # resnet-101\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "train_dataset = CloudImageDataSet('./cloud_dataset/images/', train_image_name_and_label_set, data_transforms, 'Train')\n",
    "val_dataset = CloudImageDataSet('./cloud_dataset/images', val_image_name_and_label_set, data_transforms, 'Test')\n",
    "train_set, val_set = train_dataset, val_dataset\n",
    "train_dataloader =  DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "y_set = torch.tensor([])\n",
    "pred_set = torch.tensor([])\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "model.load_state_dict(torch.load('./model_state_dict/model-64-3.0-resnet-50-epoch13-optimizerradam-schedulercosine_annealing_warm_restarts-f1-score0.42401080509544153'))\n",
    "model.eval()\n",
    "_transforms = tta.Compose(\n",
    "    [\n",
    "        tta.HorizontalFlip(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tta_model = tta.ClassificationTTAWrapper(model, tta.aliases.five_crop_transform(224, 224))\n",
    "with torch.no_grad():\n",
    "    pred_set = torch.tensor([])\n",
    "    y_set = torch.tensor([])\n",
    "    for X, y in val_dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = tta_model(X)\n",
    "        pred_set = torch.cat((pred_set, pred.cpu()), 0)\n",
    "        y_set = torch.cat((y_set, y.cpu()), 0)\n",
    "f1 = f1_score(pred_set.argmax(1), y_set.argmax(1), average=None).sum().item() / cloud_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e861e3af-62f6-453a-b3c7-b9da130894ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47058824, 0.72222222, 0.66666667, 0.18181818, 0.22857143,\n",
       "       1.        , 0.09302326, 0.50684932, 0.66666667, 0.73187184,\n",
       "       0.32432432, 1.        , 0.45454545, 0.25      , 0.15873016,\n",
       "       0.        , 0.44776119, 0.54950495, 0.57462687, 0.4717608 ,\n",
       "       0.42105263, 0.11111111, 0.        , 0.44444444, 0.54439592,\n",
       "       0.28571429, 0.25570776, 0.31034483])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(pred_set.argmax(1), y_set.argmax(1), average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f26a1f9a-b67f-46d6-bc28-ae43c8e1e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "v, c = pred_set.argmax(1).unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fce5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "v, c = list(v), list(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a2f2e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tensor(0): tensor(9),\n",
       " tensor(1): tensor(413),\n",
       " tensor(2): tensor(6),\n",
       " tensor(3): tensor(4),\n",
       " tensor(4): tensor(13),\n",
       " tensor(5): tensor(1),\n",
       " tensor(6): tensor(13),\n",
       " tensor(7): tensor(82),\n",
       " tensor(8): tensor(1),\n",
       " tensor(9): tensor(306),\n",
       " tensor(10): tensor(13),\n",
       " tensor(11): tensor(2),\n",
       " tensor(12): tensor(9),\n",
       " tensor(13): tensor(4),\n",
       " tensor(14): tensor(23),\n",
       " tensor(16): tensor(36),\n",
       " tensor(17): tensor(202),\n",
       " tensor(18): tensor(136),\n",
       " tensor(19): tensor(133),\n",
       " tensor(20): tensor(46),\n",
       " tensor(21): tensor(19),\n",
       " tensor(22): tensor(1),\n",
       " tensor(23): tensor(7),\n",
       " tensor(24): tensor(365),\n",
       " tensor(25): tensor(2),\n",
       " tensor(26): tensor(112),\n",
       " tensor(27): tensor(85)}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(v, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57aa4d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloud-form",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1e96b12bef5a4a6fb333b9033346ea113131d381a69028321c12eff0af5df27e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
